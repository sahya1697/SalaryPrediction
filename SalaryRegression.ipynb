{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vncn_lrmYkhM"
      },
      "source": [
        "REGRESSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2w0vA4tXWzD"
      },
      "source": [
        "\n",
        "\n",
        "**1.linear regression**\n",
        "\n",
        "*    Linear regression is a statistical model that estimates the linear relationship between a scalar response (often denoted as (y)) and one or more explanatory variables (also known as independent variables, often denoted as (x))\n",
        "*   Parameters:\n",
        "    fit_interceptbool, default=True\n",
        "Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n",
        "\n",
        "    copy_Xbool, default=True\n",
        "If True, X will be copied; else, it may be overwritten.\n",
        "\n",
        "    n_jobsint, default=None\n",
        "The number of jobs to use for the computation. This will only provide speedup in case of sufficiently large problems, that is if firstly n_targets > 1 and secondly X is sparse or if positive is set to True. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.\n",
        "\n",
        "    positivebool, default=False\n",
        "When set to True, forces the coefficients to be positive. This option is only supported for dense arrays.\n",
        "\n",
        "\n",
        "**2.knn regression**\n",
        "\n",
        "*   K-Nearest Neighbors (KNN) regression is a non-parametric algorithm that predicts the value of a target variable based on the average (or weighted average) of its (k) nearest neighbors in the feature space. It’s a simple yet effective method for regression tasks.\n",
        "*   Parameters:\n",
        "    n_neighborsint, default=5\n",
        "Number of neighbors to use by default for kneighbors queries.\n",
        "\n",
        "    weights{‘uniform’, ‘distance’}, callable or None, default=’uniform’\n",
        "Weight function used in prediction. Possible values:\n",
        "\n",
        "‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
        "\n",
        "‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
        "\n",
        "[callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
        "\n",
        "Uniform weights are used by default.\n",
        "\n",
        "    algorithm{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
        "Algorithm used to compute the nearest neighbors:\n",
        "\n",
        "‘ball_tree’ will use BallTree\n",
        "\n",
        "‘kd_tree’ will use KDTree\n",
        "\n",
        "‘brute’ will use a brute-force search.\n",
        "\n",
        "‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n",
        "\n",
        "Note: fitting on sparse input will override the setting of this parameter, using brute force.\n",
        "\n",
        "    leaf_sizeint, default=30\n",
        "Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
        "\n",
        "    pfloat, default=2\n",
        "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
        "\n",
        "    metricstr, DistanceMetric object or callable, default=’minkowski’\n",
        "Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2. See the documentation of scipy.spatial.distance and the metrics listed in distance_metrics for valid metric values.\n",
        "\n",
        "If metric is “precomputed”, X is assumed to be a distance matrix and must be square during fit. X may be a sparse graph, in which case only “nonzero” elements may be considered neighbors.\n",
        "\n",
        "If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy’s metrics, but is less efficient than passing the metric name as a string.\n",
        "\n",
        "If metric is a DistanceMetric object, it will be passed directly to the underlying computation routines.\n",
        "\n",
        "    metric_paramsdict, default=None\n",
        "Additional keyword arguments for the metric function.\n",
        "\n",
        "    n_jobsint, default=None\n",
        "The number of parallel jobs to run for neighbors search. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details. Doesn’t affect fit method.\n",
        "\n",
        "**3.Extreme Gradient boost regression**\n",
        "\n",
        "*   XGBoost (Extreme Gradient Boosting) is a powerful gradient boosting algorithm for regression tasks. It combines decision trees and gradient boosting to achieve high predictive accuracy.\n",
        "*   parameters:\n",
        "    n_estimators\n",
        "The number of boosting stages that will be performed. Later we will plot deviance against boosting iterations.\n",
        "\n",
        "    max_depth\n",
        "Limits the number of nodes in the tree. The best value depends on the interaction of the input variables.\n",
        "\n",
        "    min_samples_split\n",
        "The minimum number of samples required to split an internal node.\n",
        "**4.Random forest regression**\n",
        "\n",
        "*   random forest regressor combines multiple decision tree regressors to improve predictions by averaging their outputs. It’s an effective ensemble technique for regression tasks.\n",
        "*   parameters:\n",
        "\n",
        "    n_estimators\n",
        "Number of trees in the forest.\n",
        "    max_depth\n",
        "Maximum depth of each tree.\n",
        "    min_samples_split\n",
        "Minimum samples required to split an internal node.\n",
        "    min_samples_leaf\n",
        "Minimum samples required at a leaf node.\n",
        "\n",
        "\n",
        "**5.Ridge regression**\n",
        "\n",
        "\n",
        "*   Ridge regression is a statistical regularization technique used in linear regression models.\n",
        "*   Parameters:\n",
        "    alphafloat or array-like of shape (n_targets,)\n",
        "Constant that multiplies the L2 term, controlling regularization strength. alpha must be a non-negative float i.e. in [0, inf).\n",
        "\n",
        "    sample_weightfloat or array-like of shape (n_samples,), default=None\n",
        "Individual weights for each sample. If given a float, every sample will have the same weight. If sample_weight is not None and solver=’auto’, the solver will be set to ‘cholesky’.\n",
        "\n",
        "    solver{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’, ‘lbfgs’}, default=’auto’\n",
        "Solver to use in the computational routines. All solvers except ‘svd’ support both dense and sparse data. However, only ‘lsqr’, ‘sag’, ‘sparse_cg’, and ‘lbfgs’ support sparse input when fit_intercept is True.\n",
        "\n",
        "    max_iterint, default=None\n",
        "Maximum number of iterations for conjugate gradient solver. For the ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined by scipy.sparse.linalg. For ‘sag’ and saga solver, the default value is 1000. For ‘lbfgs’ solver, the default value is 15000.\n",
        "\n",
        "    tolfloat, default=1e-4\n",
        "Precision of the solution. Note that tol has no effect for solvers ‘svd’ and ‘cholesky’.\n",
        "\n",
        "    verboseint, default=0\n",
        "Verbosity level. Setting verbose > 0 will display additional information depending on the solver used.\n",
        "\n",
        "    positivebool, default=False\n",
        "When set to True, forces the coefficients to be positive. Only ‘lbfgs’ solver is supported in this case.\n",
        "\n",
        "    random_stateint, RandomState instance, default=None\n",
        "Used when solver == ‘sag’ or ‘saga’ to shuffle the data. See Glossary for details.\n",
        "\n",
        "    return_n_iterbool, default=False\n",
        "If True, the method also returns n_iter, the actual number of iteration performed by the solver.\n",
        "\n",
        "    return_interceptbool, default=False\n",
        "If True and if X is sparse, the method also returns the intercept, and the solver is automatically changed to ‘sag’. This is only a temporary fix for fitting the intercept with sparse data. For dense data, use sklearn.linear_model._preprocess_data before your regression.\n",
        "\n",
        "    check_inputbool, default=True\n",
        "If False, the input arrays X and y will not be checked.\n",
        "\n",
        "**6.Neural network**\n",
        "\n",
        "\n",
        "*   A neural network regressor is a machine learning model that uses interconnected layers of artificial neurons to predict continuous numeric values (regression tasks). It learns from data by adjusting weights during training.\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxggLM33aGxw"
      },
      "source": [
        "**Evaluation Matrics**\n",
        "\n",
        "1.Mean Absolute Error (MAE): This metric calculates the average absolute difference between the predicted values and the actual target values. It provides a straightforward measure of prediction accuracy. The formula for MAE is:\n",
        "\n",
        "MAE=n1​i=1∑n​∣yi​−y^​i​∣\n",
        "\n",
        "where:\n",
        "\n",
        "(n) is the number of data points.\n",
        "(y_i) represents the actual target value.\n",
        "(\\hat{y}_i) represents the predicted value.\n",
        "\n",
        "\n",
        "\n",
        "2.Mean Squared Error (MSE): MSE computes the average squared difference between predicted and actual values. It penalizes larger errors more heavily. The formula for MSE is:\n",
        "MSE=n1​i=1∑n​(yi​−y^​i​)2\n",
        "\n",
        "\n",
        "3.Root Mean Squared Error (RMSE): RMSE is the square root of MSE. It provides a measure of the typical error magnitude. The formula for RMSE is:\n",
        "RMSE=n1​i=1∑n​(yi​−y^​i​)2​\n",
        "\n",
        "\n",
        "4.R-squared (Coefficient of Determination): R-squared represents the proportion of variance in the target variable explained by the model. It ranges from 0 to 1, with higher values indicating better fit. An R-squared close to 1 suggests a good model fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "qZmZHOUM_tPp"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Assuming 'X' and 'y' are column names\n",
        "X=df.drop(['Salary'],axis=1)\n",
        "# X = df [[\"10percentage\", \"12graduation\", \"12percentage\", \"collegeGPA\", \"GraduationYear\", \"English\", \"Logical\", \"Quant\", \"Domain\", \"ComputerProgramming\", \"ElectronicsAndSemicon\", \"ComputerScience\", \"MechanicalEngg\", \"ElectricalEngg\", \"TelecomEngg\", \"CivilEngg\", \"conscientiousness\", \"agreeableness\", \"extraversion\", \"nueroticism\", \"openess_to_experience\"]]\n",
        "#\"ID\", \"Gender\", \"DOB\", , \"10board\" \"12board\", \"CollegeID\", \"CollegeTier\", \"Specialization\", \"CollegeState\",\"Degree\",\n",
        "y = df[\"Salary\"].values  # Target vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-Nzv2nqPABux"
      },
      "outputs": [],
      "source": [
        "#NORMALIZATION\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to your data and transform it\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5bwWwQk_y4i",
        "outputId": "244bd3d8-497c-452f-ffb1-ee556f97ce20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Absolute Error (MAE): 0.51\n",
            "Mean Squared Error (MSE): 0.67\n",
            "Root Mean Squared Error (RMSE): 0.82\n",
            "R-squared: 0.16\n"
          ]
        }
      ],
      "source": [
        "#LINEAR REGRESSION\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model (you can use R-squared or other metrics)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "aaXP6Pl76JHv"
      },
      "outputs": [],
      "source": [
        "# Example prediction\n",
        "# sample_input = X_test.iloc[0].values.reshape(1, -1)  # Use .iloc to access row by index and .values to get NumPy array\n",
        "# predicted_value = model.predict(sample_input)\n",
        "# print(f\"Predicted value: {predicted_value[0]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y43t09Wqlz9Q",
        "outputId": "b94711f8-f965-4782-bb3d-df1d93302030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: {'fit_intercept': False}\n",
            "MAE: 0.51\n",
            "RMSE: 0.82\n",
            "MSE: 0.67\n",
            "R-squared: 0.16\n"
          ]
        }
      ],
      "source": [
        "#LINEAR REGRESSION\n",
        "#grid search\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'fit_intercept': [True, False],\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {grid_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPq8RDgjBREZ",
        "outputId": "75fa8c7e-d380-418f-e917-f78c6dd0857f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: {'fit_intercept': False, 'copy_X': True}\n",
            "MAE: 0.51\n",
            "RMSE: 0.82\n",
            "MSE: 0.67\n",
            "R-squared: 0.16\n"
          ]
        }
      ],
      "source": [
        "#LINEAR REGRESSION\n",
        "#random search\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define hyperparameters to search (adjust as needed)\n",
        "# Removed 'normalize' as it's not a valid parameter for LinearRegression\n",
        "param_dist = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'copy_X': [True, False]\n",
        "}\n",
        "\n",
        "# Perform random search with cross-validation\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=3, cv=5,\n",
        "                                   scoring='neg_mean_squared_error', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {random_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVIJxpvrSikE",
        "outputId": "0c9d715d-6927-40ce-b89a-25466d6f6106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.5.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-24.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Collecting colorama<0.5.0,>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bayesian_optimization-1.5.1-py3-none-any.whl (28 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading pyaml-24.7.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyaml, colorama, scikit-optimize, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.5.1 colorama-0.4.6 pyaml-24.7.0 scikit-optimize-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn scikit-optimize matplotlib bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL8lfIHjCTnj",
        "outputId": "81cdb7b5-c226-4825-a281-d17559b377a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: LinearRegression(fit_intercept=False)\n",
            "MAE: 0.51\n",
            "RMSE: 0.82\n",
            "MSE: 0.67\n",
            "R-squared: 0.16\n"
          ]
        }
      ],
      "source": [
        "#LINEAR REGRESSION\n",
        "#bayesian optimization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Load your dataset and preprocess it (replace with your data)\n",
        "# Split into train and test sets\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define hyperparameters to search (adjust as needed)\n",
        "# Removed 'normalize' as it's not a valid parameter for LinearRegression\n",
        "param_dist = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'copy_X': [True, False]\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization with cross-validation\n",
        "# Use 'search_spaces' instead of 'param_distributions'\n",
        "bayes_search = BayesSearchCV(model, search_spaces=param_dist, n_iter=10, cv=5,\n",
        "                             scoring='neg_mean_squared_error', random_state=42)\n",
        "bayes_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = bayes_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {bayes_search.best_estimator_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cjZclYjAkvM",
        "outputId": "a16c4fba-70e4-42e7-87eb-8b95f41d6258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Absolute Error (MAE): 0.62\n",
            "Mean Squared Error (MSE): 1.10\n",
            "Root Mean Squared Error (RMSE): 1.05\n",
            "R-squared: -0.38\n"
          ]
        }
      ],
      "source": [
        "#KNN REGRESSOR\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor # Use KNeighborsRegressor for regression\n",
        "\n",
        "\n",
        "# Create and fit the kNN model\n",
        "knn = KNeighborsRegressor(n_neighbors=3) # Changed to KNeighborsRegressor\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9efuwscDzPK",
        "outputId": "2646c176-f893-4aaa-a868-648d84c164ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: {'n_neighbors': 9, 'weights': 'distance'}\n",
            "MAE: 0.55\n",
            "RMSE: 0.87\n",
            "MSE: 0.75\n",
            "R-squared: 0.06\n"
          ]
        }
      ],
      "source": [
        "#KNN REGRESSOR\n",
        "#gridsearch\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsRegressor # Use KNeighborsRegressor for regression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the kNN regressor model\n",
        "model = KNeighborsRegressor() # Changed to KNeighborsRegressor\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],  # Example: search for number of neighbors\n",
        "    'weights': ['uniform', 'distance']  # Example: search for weight function\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {grid_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkdfY4ydEjG0",
        "outputId": "da3e912b-4d93-4ceb-b5b1-9642c3776e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: {'weights': 'distance', 'n_neighbors': 9}\n",
            "MAE: 0.55\n",
            "RMSE: 0.87\n",
            "MSE: 0.75\n",
            "R-squared: 0.06\n"
          ]
        }
      ],
      "source": [
        "#KNN REGRESSOR\n",
        "#random search\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the kNN regressor model\n",
        "model = KNeighborsRegressor()\n",
        "\n",
        "# Define hyperparameters to search (adjust as needed)\n",
        "param_dist = {\n",
        "    'n_neighbors': [3, 5, 7, 9],  # Example: search for number of neighbors\n",
        "    'weights': ['uniform', 'distance']  # Example: search for weight function\n",
        "}\n",
        "\n",
        "# Perform random search with cross-validation\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=7, cv=5,\n",
        "                                   scoring='neg_mean_squared_error', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {random_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDy0qMbRFM9J",
        "outputId": "b91d8d58-9259-4a42-e1d9-7a7b6a880f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: OrderedDict([('n_neighbors', 9), ('weights', 'distance')])\n",
            "MAE: 0.55\n",
            "RMSE: 0.87\n",
            "MSE: 0.75\n",
            "R-squared: 0.06\n"
          ]
        }
      ],
      "source": [
        "#KNN REGRESSOR\n",
        "#bayesian optimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the kNN regressor model\n",
        "model = KNeighborsRegressor()\n",
        "\n",
        "# Define hyperparameters to search (adjust as needed)\n",
        "# Use 'search_spaces' instead of 'param_distributions' for BayesSearchCV\n",
        "param_dist = {\n",
        "    'n_neighbors': (3, 9),  # Example: search range for number of neighbors\n",
        "    'weights': ['uniform', 'distance']  # Example: search for weight function\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization with cross-validation\n",
        "bayes_search = BayesSearchCV(model, search_spaces=param_dist, n_iter=10, cv=5, # Changed 'param_distributions' to 'search_spaces'\n",
        "                             scoring='neg_mean_squared_error', random_state=42)\n",
        "bayes_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = bayes_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {bayes_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXbjkAO1AWRa",
        "outputId": "3fc019d0-c8b9-4c26-a6f0-fb8d3bbbb45b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Absolute Error (MAE): 0.53\n",
            "Mean Squared Error (MSE): 0.74\n",
            "Root Mean Squared Error (RMSE): 0.86\n",
            "R-squared: 0.08\n"
          ]
        }
      ],
      "source": [
        "#XGBOOST REGRESSOR\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "\n",
        "\n",
        "\n",
        "# Create an XGBoost regressor\n",
        "xg_reg = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Fit the model\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Make predictions\n",
        "y_pred = xg_reg.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model (you can use metrics like RMSE or R-squared)\n",
        "# Example: RMSE\n",
        "\n",
        "\n",
        "# Example prediction for a new data point\n",
        "# new_data_point = X_test.iloc[0].values.reshape(1, -1)\n",
        "# predicted_price = xg_reg.predict(new_data_point)\n",
        "#print(f\"Predicted price: ${predicted_price[0]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G_CLJnOqSGD",
        "outputId": "b303cab3-e2cd-4892-ffd6-cdab7f8b5b06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x7937c815ea10>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 637, in _next_wrapper\n",
            "    return self._handle_exception(lambda: self.next(input_data), 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 550, in _handle_exception\n",
            "    return fn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 637, in <lambda>\n",
            "    return self._handle_exception(lambda: self.next(input_data), 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 1416, in next\n",
            "    input_data(**self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
            "    return func(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 626, in input_data\n",
            "    self.proxy.set_info(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 726, in inner_f\n",
            "    return func(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 954, in set_info\n",
            "    self.set_label(label)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 1092, in set_label\n",
            "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 1354, in dispatch_meta_backend\n",
            "    _meta_from_numpy(data, name, dtype, handle)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/data.py\", line 1295, in _meta_from_numpy\n",
            "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50}\n",
            "MAE: 0.53\n",
            "RMSE: 0.87\n",
            "R-squared: 0.05\n"
          ]
        }
      ],
      "source": [
        "#XGBOOST REGRESSOR\n",
        "#gridsearch\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the XGBoost regressor\n",
        "xg_reg = XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Adjust the number of trees\n",
        "    'max_depth': [3, 5, 7],  # Explore different tree depths\n",
        "    'learning_rate': [0.01, 0.1, 0.2]  # Try different learning rates\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(xg_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_xg_reg = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_xg_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {grid_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru3fvBHmGY8u",
        "outputId": "c1380767-8dfd-448a-8846-acc82b44d5ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1}\n",
            "MAE: 0.53\n",
            "RMSE: 0.85\n",
            "R-squared: 0.10\n"
          ]
        }
      ],
      "source": [
        "#XGBOOST REGRESSOR\n",
        "#randomsearch\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the XGBoost regressor\n",
        "xg_reg = XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Define hyperparameters to search (adjust as needed)\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],  # Adjust the number of trees\n",
        "    'max_depth': [3, 5, 7],  # Explore different tree depths\n",
        "    'learning_rate': [0.01, 0.1, 0.2]  # Try different learning rates\n",
        "}\n",
        "\n",
        "# Perform random search with cross-validation\n",
        "random_search = RandomizedSearchCV(xg_reg, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='neg_mean_squared_error', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_xg_reg = random_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_xg_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {random_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK-nnYUxdPGb",
        "outputId": "6d93faaf-255e-44b6-c0b2-4ac8564bf028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: OrderedDict([('learning_rate', 0.16435523778789904), ('max_depth', 4), ('n_estimators', 140)])\n",
            "MAE: 0.55\n",
            "RMSE: 0.86\n",
            "R-squared: 0.07\n"
          ]
        }
      ],
      "source": [
        "#XGBOOST REGRESSOR\n",
        "# bayesian optimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the XGBoost regressor\n",
        "xg_reg = XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "\n",
        "# Define hyperparameters to search (adjust as needed)\n",
        "param_dist = {\n",
        "    'n_estimators': (50, 200),  # Adjust the range for the number of trees\n",
        "    'max_depth': (3, 7),  # Explore different tree depths\n",
        "    'learning_rate': (0.01, 0.2)  # Try different learning rates\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization with cross-validation\n",
        "# Use 'search_spaces' instead of 'param_distributions'\n",
        "bayes_search = BayesSearchCV(xg_reg, search_spaces=param_dist, n_iter=10, cv=5,\n",
        "                             scoring='neg_mean_squared_error', random_state=42)\n",
        "\n",
        "# Handle potential errors during fitting\n",
        "try:\n",
        "    bayes_search.fit(X_train, y_train)\n",
        "except XGBoostError as e:\n",
        "    print(f\"XGBoostError occurred: {e}\")\n",
        "    # Add debugging or handling logic here, such as inspecting the data or adjusting hyperparameters\n",
        "\n",
        "# Get the best model if fitting was successful\n",
        "if hasattr(bayes_search, 'best_estimator_'):\n",
        "    best_xg_reg = bayes_search.best_estimator_\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = best_xg_reg.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Best Model Parameters: {bayes_search.best_params_}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"R-squared: {r2:.2f}\")\n",
        "else:\n",
        "    print(\"Bayesian optimization failed to find a suitable model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1wQQ2mJC4DS",
        "outputId": "e274e4b2-bf4e-4244-fbb8-3a2c8a005cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Absolute Error (MAE): 0.51\n",
            "Mean Squared Error (MSE): 0.78\n",
            "Root Mean Squared Error (RMSE): 0.88\n",
            "R-squared: 0.06\n"
          ]
        }
      ],
      "source": [
        "#RANDOM FOREST REGRESSOR\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create and train the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train.ravel())\n",
        "\n",
        "xg_reg.fit(X_train, y_train)\n",
        "# Make predictions\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2:.2f}\")\n",
        "\n",
        "\n",
        "# Example prediction for a new data point\n",
        "#new_data_point = X_test.iloc[0].values.reshape(1, -1)\n",
        "#predicted_price = rf_regressor.predict(new_data_point)\n",
        "#print(f\"Predicted price: ${predicted_price[0]:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbtWjnN_wIDy",
        "outputId": "251505fa-0cc8-4fdc-9b17-2d9efbb0a2c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50}\n",
            "MAE: 0.57\n",
            "RMSE: 0.91\n",
            "R-squared: -0.04\n"
          ]
        }
      ],
      "source": [
        "#RANDOM FOREST REGRESSOR\n",
        "#gridsearch\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Adjust the number of trees\n",
        "    'max_depth': [5, 10, 15],  # Explore different tree depths\n",
        "    'min_samples_split': [2, 5, 10]  # Minimum samples required to split a node\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(rf_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get the best model\n",
        "best_rf_regressor = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_rf_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {grid_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfUEYw46KhFX",
        "outputId": "1b43f1e5-9c69-4697-ae7e-31fbff84a96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: {'n_estimators': 100, 'min_samples_split': 10, 'max_depth': 10}\n",
            "MAE: 0.56\n",
            "RMSE: 0.91\n",
            "R-squared: -0.03\n"
          ]
        }
      ],
      "source": [
        "#RANDOM FOREST REGRESSOR\n",
        "#random search\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "# Define hyperparameters to search (adjust as needed)\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],  # Adjust the number of trees\n",
        "    'max_depth': [5, 10, 15],  # Explore different tree depths\n",
        "    'min_samples_split': [2, 5, 10]  # Minimum samples required to split a node\n",
        "}\n",
        "\n",
        "# Perform random search with cross-validation\n",
        "random_search = RandomizedSearchCV(rf_regressor, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='neg_mean_squared_error', random_state=42)\n",
        "random_search.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get the best model\n",
        "best_rf_regressor = random_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_rf_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {random_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b6K-jKvOckY",
        "outputId": "2f628317-33c5-463f-c380-9a208803d67d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Parameters: OrderedDict([('max_depth', 9), ('min_samples_split', 9), ('n_estimators', 66)])\n",
            "MAE: 0.57\n",
            "RMSE: 0.91\n",
            "R-squared: -0.04\n"
          ]
        }
      ],
      "source": [
        "#RANDOM FOREST REGRESSOR\n",
        "#bayesian optimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "# Define hyperparameters to search (adjust as needed)\n",
        "param_dist = {\n",
        "    'n_estimators': (50, 200),  # Adjust the range for the number of trees\n",
        "    'max_depth': (5, 15),  # Explore different tree depths\n",
        "    'min_samples_split': (2, 10)  # Minimum samples required to split a node\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization with cross-validation\n",
        "# Use 'search_spaces' instead of 'param_distributions' for BayesSearchCV\n",
        "bayes_search = BayesSearchCV(rf_regressor, search_spaces=param_dist, n_iter=10, cv=5,\n",
        "                             scoring='neg_mean_squared_error', random_state=42)\n",
        "bayes_search.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get the best model\n",
        "best_rf_regressor = bayes_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_rf_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Model Parameters: {bayes_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrwbck_rpaWO",
        "outputId": "f5b086d9-6d08-4936-a472-5a5d467d8a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE: 0.51\n",
            "MSE: 0.67\n",
            "RMSE: 0.82\n",
            "R-squared: 0.16\n"
          ]
        }
      ],
      "source": [
        "#RIDGE REGRESSION\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# Define cross-validation method\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# Fit the ridge regression model\n",
        "model = Ridge(alpha=0.99)  # Use the optimal alpha value from previous steps\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSoAU6F7_mmg",
        "outputId": "7fd1e3a0-1bc8-4017-e814-10daebe06eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Ridge Model Parameters: {'alpha': 100}\n",
            "MAE: 0.51\n",
            "RMSE: 0.82\n",
            "R-squared: 0.16\n"
          ]
        }
      ],
      "source": [
        "#RIDGE REGRESSION\n",
        "#grid search\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {'alpha': [1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]}\n",
        "\n",
        "# Create Ridge regression model\n",
        "ridge_model = Ridge()\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "ridge_grid_search = GridSearchCV(ridge_model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "ridge_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_ridge_model = ridge_grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_ridge_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Ridge Model Parameters: {ridge_grid_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-udYxSSPpSi",
        "outputId": "a3bc8c86-53b7-4025-849d-bfa876825857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Ridge Model Parameters: {'alpha': 0.1788649529057435}\n",
            "MAE: 0.51\n",
            "RMSE: 0.82\n",
            "R-squared: 0.16\n"
          ]
        }
      ],
      "source": [
        "#RIDGE REGRESSION\n",
        "#randomsearch\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Ridge regression model\n",
        "ridge_model = Ridge()\n",
        "\n",
        "# Define hyperparameters to search (adjust as needed)\n",
        "param_dist = {\n",
        "    'alpha': np.logspace(-15, 2, num=100)  # Adjust the range for alpha\n",
        "}\n",
        "\n",
        "# Perform random search with cross-validation\n",
        "random_search = RandomizedSearchCV(ridge_model, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='neg_mean_squared_error', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_ridge_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_ridge_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Ridge Model Parameters: {random_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVRygd26QIIO",
        "outputId": "9fc30d73-ad4b-43f8-ec85-73e73f39888b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Bayesian Ridge Model Parameters: {'lambda_2': 0.00026560877829466864, 'lambda_1': 0.04862601580065353, 'alpha_2': 0.7924828983539186, 'alpha_1': 0.0001261856883066021}\n",
            "MAE: 0.51\n",
            "RMSE: 0.82\n",
            "R-squared: 0.16\n"
          ]
        }
      ],
      "source": [
        "#RIDGE REGRESSION\n",
        "#bayesian optimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Bayesian Ridge regression model\n",
        "bayesian_ridge_model = BayesianRidge()\n",
        "\n",
        "# Define hyperparameters to search (adjust as needed)\n",
        "param_dist = {\n",
        "    'alpha_1': np.logspace(-6, 2, num=100),  # Adjust the range for alpha_1\n",
        "    'alpha_2': np.logspace(-6, 2, num=100),  # Adjust the range for alpha_2\n",
        "    'lambda_1': np.logspace(-6, 2, num=100),  # Adjust the range for lambda_1\n",
        "    'lambda_2': np.logspace(-6, 2, num=100)  # Adjust the range for lambda_2\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization with cross-validation\n",
        "bayesian_search = RandomizedSearchCV(bayesian_ridge_model, param_distributions=param_dist,\n",
        "                                     n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "bayesian_search.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get the best model\n",
        "best_bayesian_ridge_model = bayesian_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_bayesian_ridge_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Bayesian Ridge Model Parameters: {bayesian_search.best_params_}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxMyWX7J1DHv",
        "outputId": "be78ec4f-a1d8-4a39-edb2-5e5d345ae762"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:1563: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 914.5956359105978, tolerance: 0.21581486255071677\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 950.8266901864663, tolerance: 0.22398134593500682\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 804.8748795140881, tolerance: 0.1941975243606846\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 968.2960348513125, tolerance: 0.22695703803071576\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 915.1822484326992, tolerance: 0.21471900183379358\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 817.993579143437, tolerance: 0.19622567551811185\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 921.5540208431684, tolerance: 0.21673041023137873\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 923.4057217086627, tolerance: 0.21589390671346564\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 958.2618934036784, tolerance: 0.2251190239575929\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 847.7685288616879, tolerance: 0.199392989022402\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 961.4063524892374, tolerance: 0.2252290994488515\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 919.0986590776477, tolerance: 0.21922554865855695\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 945.9718237198483, tolerance: 0.2227168951980499\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 953.9509169009556, tolerance: 0.22492902622792563\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 912.3151213121679, tolerance: 0.21325084144929998\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 920.0408696226943, tolerance: 0.21797613633686105\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 961.7836068884534, tolerance: 0.22378493179090275\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 901.6222466358627, tolerance: 0.2105960083077338\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 844.9187398248473, tolerance: 0.1980161045742551\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 704.5165374258622, tolerance: 0.17341435124977292\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 856.0551879473068, tolerance: 0.20427339942801215\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 957.6944398007224, tolerance: 0.22442514956266674\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 960.4222635710569, tolerance: 0.22520880590076067\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 763.1101245584949, tolerance: 0.18156011877872097\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 964.4666208876221, tolerance: 0.22563218899682025\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 917.2697469171306, tolerance: 0.21497074882560274\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 880.0849458791279, tolerance: 0.20921916665186843\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal alpha (lambda): 0.01\n",
            "Mean Absolute Error (MAE): 0.51\n",
            "Mean Squared Error (MSE): 0.67\n",
            "Root Mean Squared Error (RMSE): 0.82\n",
            "R-squared: 0.16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 931.1015086702689, tolerance: 0.21917945529142016\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 970.2789951844493, tolerance: 0.22796145846106483\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 823.3008806778843, tolerance: 0.19664285747003218\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n"
          ]
        }
      ],
      "source": [
        "#lasso REgression\n",
        "import pandas as pd\n",
        "from numpy import arange\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "model = LassoCV(alphas=arange(0, 1, 0.01), cv=cv, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Optimal alpha (lambda): {model.alpha_:.2f}\")\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5InKHnz3Me6",
        "outputId": "bc05c3eb-982e-4499-fdde-e1da778c7a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best alpha: 0.0100\n",
            "Best L1 ratio: 0.60\n",
            "Mean Absolute Error (MAE): 0.51\n",
            "Mean Squared Error (MSE): 0.67\n",
            "Root Mean Squared Error (RMSE): 0.82\n",
            "R-squared: 0.16\n"
          ]
        }
      ],
      "source": [
        "#Elastic net\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler for normalization\n",
        "\n",
        "# Assuming X_train and y_train are already defined and normalized\n",
        "# If not, normalize them here:\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# y_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
        "\n",
        "# Initialize the Elastic Net model without the 'normalize' parameter\n",
        "elastic_net = ElasticNet()\n",
        "\n",
        "# Define the hyperparameter grid for grid search\n",
        "param_grid = {\n",
        "    'alpha': np.logspace(-5, 2, 8),  # Range of alpha values\n",
        "    'l1_ratio': [0.2, 0.4, 0.6, 0.8]  # L1 ratio (mixing parameter)\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "search = GridSearchCV(estimator=elastic_net, param_grid=param_grid,\n",
        "                      scoring='neg_mean_squared_error', cv=10, n_jobs=1, refit=True)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_alpha = search.best_params_['alpha']\n",
        "best_l1_ratio = search.best_params_['l1_ratio']\n",
        "\n",
        "# Create the final Elastic Net model with the best hyperparameters\n",
        "final_elastic_net = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio) # Remove normalize=True\n",
        "final_elastic_net.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best alpha: {best_alpha:.4f}\")\n",
        "print(f\"Best L1 ratio: {best_l1_ratio:.2f}\")\n",
        "\n",
        "y_pred = final_elastic_net.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnmxeg2dOp1p",
        "outputId": "e64d741e-1cb7-476b-b857-6e2b1a3e90d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust Regression (RANSAC) Metrics:\n",
            "Mean Absolute Error (MAE): 0.57\n",
            "Mean Squared Error (MSE): 0.82\n",
            "Root Mean Squared Error (RMSE): 0.91\n",
            "R-squared: -0.03\n"
          ]
        }
      ],
      "source": [
        "#Robust Regression\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import RANSACRegressor, LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the robust regression model using RANSAC\n",
        "# For older scikit-learn versions, you don't need to specify 'base_estimator'\n",
        "ransac_model = RANSACRegressor(random_state=42)\n",
        "ransac_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the robust model\n",
        "y_pred_ransac = ransac_model.predict(X_test)\n",
        "mae_ransac = mean_absolute_error(y_test, y_pred_ransac)\n",
        "mse_ransac = mean_squared_error(y_test, y_pred_ransac)\n",
        "rmse_ransac = np.sqrt(mse_ransac)\n",
        "r2_ransac = r2_score(y_test, y_pred_ransac)\n",
        "\n",
        "print(f\"Robust Regression (RANSAC) Metrics:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_ransac:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_ransac:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_ransac:.2f}\")\n",
        "print(f\"R-squared: {r2_ransac:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw8QavshO5tJ",
        "outputId": "fcbb381a-0dbf-411f-ac45-87b8059a2216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best alpha: 0.0100\n",
            "Best L1 ratio: 0.60\n",
            "Mean Absolute Error (MAE): 0.51\n",
            "Mean Squared Error (MSE): 0.67\n",
            "Root Mean Squared Error (RMSE): 0.82\n",
            "R-squared: 0.16\n"
          ]
        }
      ],
      "source": [
        "#Elastic net\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler for normalization\n",
        "\n",
        "# Assuming X and y are defined somewhere above\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data (if needed)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)  # Use the same scaler fitted on training data\n",
        "\n",
        "# Initialize the Elastic Net model without the 'normalize' parameter\n",
        "elastic_net = ElasticNet()\n",
        "\n",
        "# Define the hyperparameter grid for grid search\n",
        "param_grid = {\n",
        "    'alpha': np.logspace(-5, 2, 8),  # Range of alpha values\n",
        "    'l1_ratio': [0.2, 0.4, 0.6, 0.8]  # L1 ratio (mixing parameter)\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "search = GridSearchCV(estimator=elastic_net, param_grid=param_grid,\n",
        "                      scoring='neg_mean_squared_error', cv=10, n_jobs=1, refit=True)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_alpha = search.best_params_['alpha']\n",
        "best_l1_ratio = search.best_params_['l1_ratio']\n",
        "\n",
        "# Create the final Elastic Net model with the best hyperparameters\n",
        "final_elastic_net = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio) # Remove normalize=True\n",
        "final_elastic_net.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best alpha: {best_alpha:.4f}\")\n",
        "print(f\"Best L1 ratio: {best_l1_ratio:.2f}\")\n",
        "\n",
        "# Make predictions on the test set (remember to normalize X_test if you normalized X_train)\n",
        "y_pred = final_elastic_net.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
